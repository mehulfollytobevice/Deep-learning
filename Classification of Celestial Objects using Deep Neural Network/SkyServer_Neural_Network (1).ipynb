{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SkyServer_Neural_Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "53L3hj_mteZn",
        "colab_type": "code",
        "outputId": "5379d1b4-0cf3-46ac-eb8a-64b5e7a8025a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzuB-FfSVGDa",
        "colab_type": "code",
        "outputId": "74785c94-4ad0-497e-c6cd-3798b55d054e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "!pip3 install keras_sequential_ascii"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_sequential_ascii\n",
            "  Downloading https://files.pythonhosted.org/packages/2d/a4/806e3ed5d7ac7463e2fae77e09ccccc88c78266b248fb637e4efa4f65ec0/keras_sequential_ascii-0.1.1.tar.gz\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras_sequential_ascii) (2.3.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (1.18.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (1.4.1)\n",
            "Building wheels for collected packages: keras-sequential-ascii\n",
            "  Building wheel for keras-sequential-ascii (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-sequential-ascii: filename=keras_sequential_ascii-0.1.1-cp36-none-any.whl size=3062 sha256=bfb79a28d34222464504ef7f2e0f4fa0a6d3736c58ccb632e15a87802e9f328a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f5/8d/81/912666dff82a923ce423a7e797cd75f54271c7031512cdb282\n",
            "Successfully built keras-sequential-ascii\n",
            "Installing collected packages: keras-sequential-ascii\n",
            "Successfully installed keras-sequential-ascii-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgInGPPjunEa",
        "colab_type": "text"
      },
      "source": [
        "#Importing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuNUF9kwurlE",
        "colab_type": "text"
      },
      "source": [
        "Import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5Oe_6PeulJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "style.use('ggplot')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66t1hTlTwP6n",
        "colab_type": "text"
      },
      "source": [
        "Import the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NtCH9n1wO-5",
        "colab_type": "code",
        "outputId": "72853519-b841-47d5-95ce-e033be16b017",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "dataset=pd.read_csv(\"Skyserver_12_30_2019 4_49_58 PM.csv\")\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>objid</th>\n",
              "      <th>ra</th>\n",
              "      <th>dec</th>\n",
              "      <th>u</th>\n",
              "      <th>g</th>\n",
              "      <th>r</th>\n",
              "      <th>i</th>\n",
              "      <th>z</th>\n",
              "      <th>run</th>\n",
              "      <th>rerun</th>\n",
              "      <th>camcol</th>\n",
              "      <th>field</th>\n",
              "      <th>specobjid</th>\n",
              "      <th>class</th>\n",
              "      <th>redshift</th>\n",
              "      <th>plate</th>\n",
              "      <th>mjd</th>\n",
              "      <th>fiberid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1237666301628060000</td>\n",
              "      <td>47.372545</td>\n",
              "      <td>0.820621</td>\n",
              "      <td>18.69254</td>\n",
              "      <td>17.13867</td>\n",
              "      <td>16.55555</td>\n",
              "      <td>16.34662</td>\n",
              "      <td>16.17639</td>\n",
              "      <td>4849</td>\n",
              "      <td>301</td>\n",
              "      <td>5</td>\n",
              "      <td>771</td>\n",
              "      <td>8168632633242440000</td>\n",
              "      <td>STAR</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>7255</td>\n",
              "      <td>56597</td>\n",
              "      <td>832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1237673706652430000</td>\n",
              "      <td>116.303083</td>\n",
              "      <td>42.455980</td>\n",
              "      <td>18.47633</td>\n",
              "      <td>17.30546</td>\n",
              "      <td>17.24116</td>\n",
              "      <td>17.32780</td>\n",
              "      <td>17.37114</td>\n",
              "      <td>6573</td>\n",
              "      <td>301</td>\n",
              "      <td>6</td>\n",
              "      <td>220</td>\n",
              "      <td>9333948945297330000</td>\n",
              "      <td>STAR</td>\n",
              "      <td>-0.000093</td>\n",
              "      <td>8290</td>\n",
              "      <td>57364</td>\n",
              "      <td>868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1237671126974140000</td>\n",
              "      <td>172.756623</td>\n",
              "      <td>-8.785698</td>\n",
              "      <td>16.47714</td>\n",
              "      <td>15.31072</td>\n",
              "      <td>15.55971</td>\n",
              "      <td>15.72207</td>\n",
              "      <td>15.82471</td>\n",
              "      <td>5973</td>\n",
              "      <td>301</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>3221211255238850000</td>\n",
              "      <td>STAR</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>2861</td>\n",
              "      <td>54583</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1237665441518260000</td>\n",
              "      <td>201.224207</td>\n",
              "      <td>28.771290</td>\n",
              "      <td>18.63561</td>\n",
              "      <td>16.88346</td>\n",
              "      <td>16.09825</td>\n",
              "      <td>15.70987</td>\n",
              "      <td>15.43491</td>\n",
              "      <td>4649</td>\n",
              "      <td>301</td>\n",
              "      <td>3</td>\n",
              "      <td>121</td>\n",
              "      <td>2254061292459420000</td>\n",
              "      <td>GALAXY</td>\n",
              "      <td>0.058155</td>\n",
              "      <td>2002</td>\n",
              "      <td>53471</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1237665441522840000</td>\n",
              "      <td>212.817222</td>\n",
              "      <td>26.625225</td>\n",
              "      <td>18.88325</td>\n",
              "      <td>17.87948</td>\n",
              "      <td>17.47037</td>\n",
              "      <td>17.17441</td>\n",
              "      <td>17.05235</td>\n",
              "      <td>4649</td>\n",
              "      <td>301</td>\n",
              "      <td>3</td>\n",
              "      <td>191</td>\n",
              "      <td>2390305906828010000</td>\n",
              "      <td>GALAXY</td>\n",
              "      <td>0.072210</td>\n",
              "      <td>2123</td>\n",
              "      <td>53793</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99995</th>\n",
              "      <td>1237667968033620000</td>\n",
              "      <td>228.674917</td>\n",
              "      <td>19.179460</td>\n",
              "      <td>19.32631</td>\n",
              "      <td>18.82748</td>\n",
              "      <td>18.65659</td>\n",
              "      <td>18.60481</td>\n",
              "      <td>18.60917</td>\n",
              "      <td>5237</td>\n",
              "      <td>301</td>\n",
              "      <td>5</td>\n",
              "      <td>134</td>\n",
              "      <td>4448615345201370000</td>\n",
              "      <td>QSO</td>\n",
              "      <td>0.438182</td>\n",
              "      <td>3951</td>\n",
              "      <td>55681</td>\n",
              "      <td>672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99996</th>\n",
              "      <td>1237664818210470000</td>\n",
              "      <td>173.757382</td>\n",
              "      <td>36.441603</td>\n",
              "      <td>18.33687</td>\n",
              "      <td>17.30365</td>\n",
              "      <td>17.16037</td>\n",
              "      <td>17.14895</td>\n",
              "      <td>17.14419</td>\n",
              "      <td>4504</td>\n",
              "      <td>301</td>\n",
              "      <td>2</td>\n",
              "      <td>111</td>\n",
              "      <td>2265404129658560000</td>\n",
              "      <td>STAR</td>\n",
              "      <td>-0.000497</td>\n",
              "      <td>2012</td>\n",
              "      <td>53493</td>\n",
              "      <td>340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99997</th>\n",
              "      <td>1237664295297290000</td>\n",
              "      <td>205.426531</td>\n",
              "      <td>38.499053</td>\n",
              "      <td>17.50690</td>\n",
              "      <td>15.63152</td>\n",
              "      <td>15.22328</td>\n",
              "      <td>15.04469</td>\n",
              "      <td>15.28668</td>\n",
              "      <td>4382</td>\n",
              "      <td>301</td>\n",
              "      <td>4</td>\n",
              "      <td>97</td>\n",
              "      <td>2257446413900210000</td>\n",
              "      <td>GALAXY</td>\n",
              "      <td>0.004587</td>\n",
              "      <td>2005</td>\n",
              "      <td>53472</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99998</th>\n",
              "      <td>1237656537513130000</td>\n",
              "      <td>337.135144</td>\n",
              "      <td>-9.635967</td>\n",
              "      <td>19.33946</td>\n",
              "      <td>17.21436</td>\n",
              "      <td>16.29697</td>\n",
              "      <td>15.86745</td>\n",
              "      <td>15.51556</td>\n",
              "      <td>2576</td>\n",
              "      <td>301</td>\n",
              "      <td>2</td>\n",
              "      <td>105</td>\n",
              "      <td>811847537492257000</td>\n",
              "      <td>GALAXY</td>\n",
              "      <td>0.084538</td>\n",
              "      <td>721</td>\n",
              "      <td>52228</td>\n",
              "      <td>268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99999</th>\n",
              "      <td>1237670459113730000</td>\n",
              "      <td>37.440188</td>\n",
              "      <td>25.400888</td>\n",
              "      <td>18.11769</td>\n",
              "      <td>16.86279</td>\n",
              "      <td>16.31057</td>\n",
              "      <td>16.07582</td>\n",
              "      <td>15.94701</td>\n",
              "      <td>5817</td>\n",
              "      <td>301</td>\n",
              "      <td>5</td>\n",
              "      <td>120</td>\n",
              "      <td>2678586035360790000</td>\n",
              "      <td>STAR</td>\n",
              "      <td>-0.000085</td>\n",
              "      <td>2379</td>\n",
              "      <td>53762</td>\n",
              "      <td>255</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100000 rows × 18 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                     objid          ra        dec  ...  plate    mjd  fiberid\n",
              "0      1237666301628060000   47.372545   0.820621  ...   7255  56597      832\n",
              "1      1237673706652430000  116.303083  42.455980  ...   8290  57364      868\n",
              "2      1237671126974140000  172.756623  -8.785698  ...   2861  54583       42\n",
              "3      1237665441518260000  201.224207  28.771290  ...   2002  53471       35\n",
              "4      1237665441522840000  212.817222  26.625225  ...   2123  53793       74\n",
              "...                    ...         ...        ...  ...    ...    ...      ...\n",
              "99995  1237667968033620000  228.674917  19.179460  ...   3951  55681      672\n",
              "99996  1237664818210470000  173.757382  36.441603  ...   2012  53493      340\n",
              "99997  1237664295297290000  205.426531  38.499053  ...   2005  53472       62\n",
              "99998  1237656537513130000  337.135144  -9.635967  ...    721  52228      268\n",
              "99999  1237670459113730000   37.440188  25.400888  ...   2379  53762      255\n",
              "\n",
              "[100000 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TXB_5_UZ6bf",
        "colab_type": "text"
      },
      "source": [
        "Dropping unnecessary columns which do not contribute to the classification of a celestial object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS6WFKHMwfjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.drop(columns=['objid','specobjid','fiberid','rerun','mjd','run','camcol','field','plate'],inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlFjLX11btZo",
        "colab_type": "code",
        "outputId": "3ad56fd4-ae54-4d95-94a2-b0cfce4fb796",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ra</th>\n",
              "      <th>dec</th>\n",
              "      <th>u</th>\n",
              "      <th>g</th>\n",
              "      <th>r</th>\n",
              "      <th>i</th>\n",
              "      <th>z</th>\n",
              "      <th>class</th>\n",
              "      <th>redshift</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>47.372545</td>\n",
              "      <td>0.820621</td>\n",
              "      <td>18.69254</td>\n",
              "      <td>17.13867</td>\n",
              "      <td>16.55555</td>\n",
              "      <td>16.34662</td>\n",
              "      <td>16.17639</td>\n",
              "      <td>STAR</td>\n",
              "      <td>0.000115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>116.303083</td>\n",
              "      <td>42.455980</td>\n",
              "      <td>18.47633</td>\n",
              "      <td>17.30546</td>\n",
              "      <td>17.24116</td>\n",
              "      <td>17.32780</td>\n",
              "      <td>17.37114</td>\n",
              "      <td>STAR</td>\n",
              "      <td>-0.000093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>172.756623</td>\n",
              "      <td>-8.785698</td>\n",
              "      <td>16.47714</td>\n",
              "      <td>15.31072</td>\n",
              "      <td>15.55971</td>\n",
              "      <td>15.72207</td>\n",
              "      <td>15.82471</td>\n",
              "      <td>STAR</td>\n",
              "      <td>0.000165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>201.224207</td>\n",
              "      <td>28.771290</td>\n",
              "      <td>18.63561</td>\n",
              "      <td>16.88346</td>\n",
              "      <td>16.09825</td>\n",
              "      <td>15.70987</td>\n",
              "      <td>15.43491</td>\n",
              "      <td>GALAXY</td>\n",
              "      <td>0.058155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>212.817222</td>\n",
              "      <td>26.625225</td>\n",
              "      <td>18.88325</td>\n",
              "      <td>17.87948</td>\n",
              "      <td>17.47037</td>\n",
              "      <td>17.17441</td>\n",
              "      <td>17.05235</td>\n",
              "      <td>GALAXY</td>\n",
              "      <td>0.072210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99995</th>\n",
              "      <td>228.674917</td>\n",
              "      <td>19.179460</td>\n",
              "      <td>19.32631</td>\n",
              "      <td>18.82748</td>\n",
              "      <td>18.65659</td>\n",
              "      <td>18.60481</td>\n",
              "      <td>18.60917</td>\n",
              "      <td>QSO</td>\n",
              "      <td>0.438182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99996</th>\n",
              "      <td>173.757382</td>\n",
              "      <td>36.441603</td>\n",
              "      <td>18.33687</td>\n",
              "      <td>17.30365</td>\n",
              "      <td>17.16037</td>\n",
              "      <td>17.14895</td>\n",
              "      <td>17.14419</td>\n",
              "      <td>STAR</td>\n",
              "      <td>-0.000497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99997</th>\n",
              "      <td>205.426531</td>\n",
              "      <td>38.499053</td>\n",
              "      <td>17.50690</td>\n",
              "      <td>15.63152</td>\n",
              "      <td>15.22328</td>\n",
              "      <td>15.04469</td>\n",
              "      <td>15.28668</td>\n",
              "      <td>GALAXY</td>\n",
              "      <td>0.004587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99998</th>\n",
              "      <td>337.135144</td>\n",
              "      <td>-9.635967</td>\n",
              "      <td>19.33946</td>\n",
              "      <td>17.21436</td>\n",
              "      <td>16.29697</td>\n",
              "      <td>15.86745</td>\n",
              "      <td>15.51556</td>\n",
              "      <td>GALAXY</td>\n",
              "      <td>0.084538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99999</th>\n",
              "      <td>37.440188</td>\n",
              "      <td>25.400888</td>\n",
              "      <td>18.11769</td>\n",
              "      <td>16.86279</td>\n",
              "      <td>16.31057</td>\n",
              "      <td>16.07582</td>\n",
              "      <td>15.94701</td>\n",
              "      <td>STAR</td>\n",
              "      <td>-0.000085</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100000 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               ra        dec         u  ...         z   class  redshift\n",
              "0       47.372545   0.820621  18.69254  ...  16.17639    STAR  0.000115\n",
              "1      116.303083  42.455980  18.47633  ...  17.37114    STAR -0.000093\n",
              "2      172.756623  -8.785698  16.47714  ...  15.82471    STAR  0.000165\n",
              "3      201.224207  28.771290  18.63561  ...  15.43491  GALAXY  0.058155\n",
              "4      212.817222  26.625225  18.88325  ...  17.05235  GALAXY  0.072210\n",
              "...           ...        ...       ...  ...       ...     ...       ...\n",
              "99995  228.674917  19.179460  19.32631  ...  18.60917     QSO  0.438182\n",
              "99996  173.757382  36.441603  18.33687  ...  17.14419    STAR -0.000497\n",
              "99997  205.426531  38.499053  17.50690  ...  15.28668  GALAXY  0.004587\n",
              "99998  337.135144  -9.635967  19.33946  ...  15.51556  GALAXY  0.084538\n",
              "99999   37.440188  25.400888  18.11769  ...  15.94701    STAR -0.000085\n",
              "\n",
              "[100000 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbmrKV5jb6SQ",
        "colab_type": "text"
      },
      "source": [
        "Let us see if the dataset is balanced or not ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-U3Nvg_bvih",
        "colab_type": "code",
        "outputId": "f6022728-fd94-40e8-c655-5d8036026d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "fig=plt.figure()\n",
        "ax=fig.add_subplot(1,1,1)\n",
        "ax.hist(dataset[\"class\"],bins=3,rwidth=0.85)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([38096., 51323., 10581.]),\n",
              " array([0.        , 0.66666667, 1.33333333, 2.        ]),\n",
              " <a list of 3 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD4CAYAAAAHHSreAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAa+0lEQVR4nO3dbVBU1+HH8e8CKuKKsoA6GO2IkaZBDMQlQU0BdZMmsU0c8zBNokmNRDtmNCajrcZO86LRkKrBoJikQoytmWSmxhA7/3TaMhQYIbQYWK3aCMamEyoW3d2gqxiEvf8Xjjuag0BVnszv88o999xzz/Hcvb+9D7vYLMuyEBERuURIb3dARET6HoWDiIgYFA4iImJQOIiIiEHhICIiBoWDiIgYwnq7A9fi2LFj7ZbHxMRw8uTJHu6NyAXa/6Q3dbT/xcXFdbkdnTmIiIhB4SAiIgaFg4iIGBQOIiJiUDiIiIhB4SAiIgaFg4iIGBQOIiJiUDiIiIihX39DWm58bc880Ntd+J/9t7c7cBVCt+7u7S5IH6MzBxERMXTpzOHZZ58lPDyckJAQQkNDyc7Oxu/3k5OTw4kTJ4iNjeX555/HbrdjWRbbtm2jpqaGQYMGsXjxYuLj4wEoKSlh165dAMyZM4fMzEwAjh49Sl5eHi0tLaSkpDB//nxsNlv3jFhERDrV5ctKL730EpGRkcHXhYWFJCUlMXv2bAoLCyksLGTu3LnU1NRw/PhxcnNzqaurIz8/n7Vr1+L3+9m5cyfZ2dkArFy5EqfTid1uZ+vWrSxatIgJEybwyiuv4Ha7SUlJuf6jFRGRLrnqy0pVVVVkZGQAkJGRQVVVFQB79+4lPT0dm81GQkICZ86cwefz4Xa7mTRpEna7HbvdzqRJk3C73fh8Ppqbm0lISMBms5Genh5sS0REekeXzxzWrFkDwN13343L5aKpqYmoqCgAhg8fTlNTEwBer5eYmJjgetHR0Xi9XrxeL9HR0cFyh8PRbvnF+u0pKiqiqKgIgOzs7Mu2c9mgwsKuuEz6l/54c7c/0vvlxnG9jn9dCodf/epXOBwOmpqaePnll43fBLfZbD1yj8DlcuFyuYKvr/Sb5fo9fZH/jd4vN44e/XsODocDgGHDhpGamsqRI0cYNmwYPp8PAJ/PF7wf4XA4LuuYx+PB4XDgcDjweDzBcq/X2275xfoiItJ7Og2Hc+fO0dzcHPz3/v37GTt2LE6nk9LSUgBKS0tJTU0FwOl0UlZWhmVZ1NbWEhERQVRUFMnJyezbtw+/34/f72ffvn0kJycTFRXF4MGDqa2txbIsysrKcDqd3ThkERHpTKeXlZqamli/fj0AbW1t3HXXXSQnJzN+/HhycnIoLi4OPsoKkJKSQnV1NUuXLmXgwIEsXrwYALvdzkMPPcSqVasAePjhh7Hb7QBkZWWxZcsWWlpaSE5O1pNKIiK9zGZZltXbnbha+hvSN77++A3p/kjfkL5x6G9Ii4hIt1E4iIiIQeEgIiIGhYOIiBgUDiIiYlA4iIiIQeEgIiIGhYOIiBgUDiIiYlA4iIiIQeEgIiIGhYOIiBgUDiIiYlA4iIiIQeEgIiIGhYOIiBgUDiIiYlA4iIiIQeEgIiIGhYOIiBgUDiIiYlA4iIiIQeEgIiIGhYOIiBgUDiIiYlA4iIiIQeEgIiIGhYOIiBgUDiIiYlA4iIiIQeEgIiKGsK5WDAQCrFy5EofDwcqVK2lsbGTjxo2cPn2a+Ph4lixZQlhYGOfPn2fz5s0cPXqUoUOHsmzZMkaMGAHAhx9+SHFxMSEhIcyfP5/k5GQA3G4327ZtIxAIMHPmTGbPnt09oxURkS7p8pnDxx9/zOjRo4Ovd+zYwaxZs9i0aRNDhgyhuLgYgOLiYoYMGcKmTZuYNWsW7777LgD19fVUVFTw2muvsXr1agoKCggEAgQCAQoKCnjxxRfJycmhvLyc+vr66zxMERH5X3QpHDweD9XV1cycORMAy7I4ePAgaWlpAGRmZlJVVQXA3r17yczMBCAtLY0DBw5gWRZVVVVMnTqVAQMGMGLECEaNGsWRI0c4cuQIo0aNYuTIkYSFhTF16tRgWyIi0ju6dFnpnXfeYe7cuTQ3NwNw+vRpIiIiCA0NBcDhcOD1egHwer1ER0cDEBoaSkREBKdPn8br9TJhwoRgm5euc7H+xX/X1dW124+ioiKKiooAyM7OJiYmpv1BhYVdcZn0L//t7Q58S+j9cuO4Xse/TsPh008/ZdiwYcTHx3Pw4MFr3uC1cLlcuFyu4OuTJ0+2Wy8mJuaKy0TEpPfLjaOj419cXFyX2+k0HA4fPszevXupqamhpaWF5uZm3nnnHc6ePUtbWxuhoaF4vV4cDgdw4YzA4/EQHR1NW1sbZ8+eZejQocHyiy5d59Jyj8cTLBcRkd7R6T2Hxx9/nDfffJO8vDyWLVvGxIkTWbp0KYmJiVRWVgJQUlKC0+kEYPLkyZSUlABQWVlJYmIiNpsNp9NJRUUF58+fp7GxkYaGBm6++WbGjx9PQ0MDjY2NtLa2UlFREWxLRER6R5cfZf2mJ554go0bN/L+++8zbtw4ZsyYAcCMGTPYvHkzS5YswW63s2zZMgDGjBnDlClTeOGFFwgJCWHBggWEhFzIpqeffpo1a9YQCASYPn06Y8aMuQ5DExGRq2WzLMvq7U5crWPHjrVbrnsON462Zx7o7S58K4Ru3d3bXZDr5Hrdc9A3pEVExKBwEBERw1Xfc+jPdKmi5+hyhUj/pDMHERExKBxERMSgcBAREYPCQUREDAoHERExKBxERMSgcBAREYPCQUREDAoHERExKBxERMSgcBAREYPCQUREDAoHERExKBxERMSgcBAREYPCQUREDAoHERExKBxERMSgcBAREYPCQUREDAoHERExKBxERMSgcBAREYPCQUREDAoHERExKBxERMSgcBAREYPCQUREDAoHERExhHVWoaWlhZdeeonW1lba2tpIS0vj0UcfpbGxkY0bN3L69Gni4+NZsmQJYWFhnD9/ns2bN3P06FGGDh3KsmXLGDFiBAAffvghxcXFhISEMH/+fJKTkwFwu91s27aNQCDAzJkzmT17dveOWkREOtTpmcOAAQN46aWXWLduHb/+9a9xu93U1tayY8cOZs2axaZNmxgyZAjFxcUAFBcXM2TIEDZt2sSsWbN49913Aaivr6eiooLXXnuN1atXU1BQQCAQIBAIUFBQwIsvvkhOTg7l5eXU19d376hFRKRDnYaDzWYjPDwcgLa2Ntra2rDZbBw8eJC0tDQAMjMzqaqqAmDv3r1kZmYCkJaWxoEDB7Asi6qqKqZOncqAAQMYMWIEo0aN4siRIxw5coRRo0YxcuRIwsLCmDp1arAtERHpHZ1eVgIIBAL8/Oc/5/jx4/zgBz9g5MiRREREEBoaCoDD4cDr9QLg9XqJjo4GIDQ0lIiICE6fPo3X62XChAnBNi9d52L9i/+uq6trtx9FRUUUFRUBkJ2dTUxMTPuDCgu74jKA/3Zl0HJddDQPXaG56hnXOk/Sd3R2/OtyO12pFBISwrp16zhz5gzr16/n2LFj17zhq+FyuXC5XMHXJ0+ebLdeTEzMFZdJz9I89A+apxtHR8e/uLi4LrfzPz2tNGTIEBITE6mtreXs2bO0tbUBF84WHA4HcOGMwOPxABcuQ509e5ahQ4deVn7pOt8s93g8wbZERKR3dBoOp06d4syZM8CFJ5f279/P6NGjSUxMpLKyEoCSkhKcTicAkydPpqSkBIDKykoSExOx2Ww4nU4qKio4f/48jY2NNDQ0cPPNNzN+/HgaGhpobGyktbWVioqKYFsiItI7Or2s5PP5yMvLIxAIYFkWU6ZMYfLkydx0001s3LiR999/n3HjxjFjxgwAZsyYwebNm1myZAl2u51ly5YBMGbMGKZMmcILL7xASEgICxYsICTkQjY9/fTTrFmzhkAgwPTp0xkzZkw3DllERDpjsyzL6u1OXK0r3fvo7J5D2zMPdFeX5BtCt+6+pvU1Vz3jWudJ+o5euecgIiLfDgoHERExKBxERMSgcBAREYPCQUREDAoHERExKBxERMSgcBAREYPCQUREDAoHERExKBxERMSgcBAREYPCQUREDAoHERExKBxERMSgcBAREYPCQUREDAoHERExKBxERMSgcBAREYPCQUREDAoHERExKBxERMSgcBAREYPCQUREDAoHERExKBxERMSgcBAREYPCQUREDAoHERExKBxERMQQ1lmFkydPkpeXx1dffYXNZsPlcnH//ffj9/vJycnhxIkTxMbG8vzzz2O327Esi23btlFTU8OgQYNYvHgx8fHxAJSUlLBr1y4A5syZQ2ZmJgBHjx4lLy+PlpYWUlJSmD9/PjabrftGLSIiHer0zCE0NJR58+aRk5PDmjVr+NOf/kR9fT2FhYUkJSWRm5tLUlIShYWFANTU1HD8+HFyc3NZuHAh+fn5APj9fnbu3MnatWtZu3YtO3fuxO/3A7B161YWLVpEbm4ux48fx+12d+OQRUSkM52GQ1RUVPCT/+DBgxk9ejRer5eqqioyMjIAyMjIoKqqCoC9e/eSnp6OzWYjISGBM2fO4PP5cLvdTJo0Cbvdjt1uZ9KkSbjdbnw+H83NzSQkJGCz2UhPTw+2JSIivaPTy0qXamxs5F//+hc333wzTU1NREVFATB8+HCampoA8Hq9xMTEBNeJjo7G6/Xi9XqJjo4OljscjnbLL9ZvT1FREUVFRQBkZ2dftp3LBhUWdsVlAP/t4njl2nU0D12hueoZ1zpP0nd0dvzrcjtdrXju3Dk2bNjAT37yEyIiIi5bZrPZeuQegcvlwuVyBV+fPHmy3XoxMTFXXCY9S/PQP2iebhwdHf/i4uK63E6XnlZqbW1lw4YNfP/73+fOO+8EYNiwYfh8PgB8Ph+RkZHAhTOCSzvm8XhwOBw4HA48Hk+w3Ov1tlt+sb6IiPSeTsPBsizefPNNRo8ezQ9/+MNgudPppLS0FIDS0lJSU1OD5WVlZViWRW1tLREREURFRZGcnMy+ffvw+/34/X727dtHcnIyUVFRDB48mNraWizLoqysDKfT2U3DFRGRruj0stLhw4cpKytj7NixrFixAoDHHnuM2bNnk5OTQ3FxcfBRVoCUlBSqq6tZunQpAwcOZPHixQDY7XYeeughVq1aBcDDDz+M3W4HICsriy1bttDS0kJycjIpKSndMlgREekam2VZVm934modO3as3fLO7jm0PfNAd3VJviF06+5rWl9z1TOudZ6k7+jRew4iIvLtonAQERGDwkFERAwKBxERMSgcRETEoHAQERGDwkFERAwKBxERMSgcRETEoHAQERGDwkFERAwKBxERMSgcRETEoHAQERGDwkFERAwKBxERMSgcRETEoHAQERGDwkFERAwKBxERMSgcRETEoHAQERGDwkFERAwKBxERMSgcRETEoHAQERGDwkFERAwKBxERMSgcRETEoHAQERGDwkFERAxhnVXYsmUL1dXVDBs2jA0bNgDg9/vJycnhxIkTxMbG8vzzz2O327Esi23btlFTU8OgQYNYvHgx8fHxAJSUlLBr1y4A5syZQ2ZmJgBHjx4lLy+PlpYWUlJSmD9/PjabrZuGKyIiXdFpOGRmZnLvvfeSl5cXLCssLCQpKYnZs2dTWFhIYWEhc+fOpaamhuPHj5Obm0tdXR35+fmsXbsWv9/Pzp07yc7OBmDlypU4nU7sdjtbt25l0aJFTJgwgVdeeQW3201KSkr3jVhErru2Zx7o7S58a4Ru3d0j2+n0stKtt96K3W6/rKyqqoqMjAwAMjIyqKqqAmDv3r2kp6djs9lISEjgzJkz+Hw+3G43kyZNwm63Y7fbmTRpEm63G5/PR3NzMwkJCdhsNtLT04NtiYhI7+n0zKE9TU1NREVFATB8+HCampoA8Hq9xMTEBOtFR0fj9Xrxer1ER0cHyx0OR7vlF+tfSVFREUVFRQBkZ2dftq3LBhUWdsVlAP/twhjl+uhoHrpCc9UzNE/9R2dz1dnxr6uuKhwuZbPZeuwegcvlwuVyBV+fPHmy3XoxMTFXXCY9S/PQP2ie+o/O5qqj419cXFyXt3NVTysNGzYMn88HgM/nIzIyErhwRnBppzweDw6HA4fDgcfjCZZ7vd52yy/WFxGR3nVV4eB0OiktLQWgtLSU1NTUYHlZWRmWZVFbW0tERARRUVEkJyezb98+/H4/fr+fffv2kZycTFRUFIMHD6a2thbLsigrK8PpdF6/0YmIyFXp9LLSxo0bOXToEKdPn+anP/0pjz76KLNnzyYnJ4fi4uLgo6wAKSkpVFdXs3TpUgYOHMjixYsBsNvtPPTQQ6xatQqAhx9+OHiTOysriy1bttDS0kJycrKeVBIR6QNslmVZvd2Jq3Xs2LF2yzu756DH7nrOtT52p7nqGZqn/qOzuerVew4iInJjUziIiIhB4SAiIgaFg4iIGBQOIiJiUDiIiIhB4SAiIgaFg4iIGBQOIiJiUDiIiIhB4SAiIgaFg4iIGBQOIiJiUDiIiIhB4SAiIgaFg4iIGBQOIiJiUDiIiIhB4SAiIgaFg4iIGBQOIiJiUDiIiIhB4SAiIgaFg4iIGBQOIiJiUDiIiIhB4SAiIgaFg4iIGBQOIiJiUDiIiIhB4SAiIgaFg4iIGMJ6uwMXud1utm3bRiAQYObMmcyePbu3uyQi8q3VJ84cAoEABQUFvPjii+Tk5FBeXk59fX1vd0tE5FurT4TDkSNHGDVqFCNHjiQsLIypU6dSVVXV290SEfnW6hOXlbxeL9HR0cHX0dHR1NXVGfWKioooKioCIDs7m7i4uCu22dEy/m/v1XdWepbmqn/QPPUpHR7/uqhPnDl0lcvlIjs7m+zs7A7rrVy5sod6JGLS/ie96Xrtf30iHBwOBx6PJ/ja4/HgcDh6sUciIt9ufSIcxo8fT0NDA42NjbS2tlJRUYHT6eztbomIfGv1iXsOoaGhPP3006xZs4ZAIMD06dMZM2bMVbfncrmuY+9E/jfa/6Q3Xa/9z2ZZlnVdWhIRkRtGn7isJCIifYvCQUREDH3inkNX7Nq1iz179hASEoLNZsNut+P3+zl37hynTp1ixIgRAGRlZfHd736XFStWMHr0aJYtWxZsIy8vj0OHDhEREQHAk08+SVJSUq+MR/qWr776iu3bt1NXV8eQIUMICwvjwQcf5I477gDgnXfe4ZNPPuGNN94gJOTCZ6qSkhI+//xzFixYYLTX1tbGwoULmTFjBk888QRw4ZcAVq1axVNPPcWtt94KwMsvv8zMmTOpqqrilltu4Z577gGgrq6Ot956i+zsbMLC+s3bVLqZx+OhoKCA+vp6AoEAKSkpPPnkkwQCAd566y3+/e9/AxAREcHq1asJDw+/bB3Lsrj99tuZN29ep/tVv9jramtr+fTTT3n11VcZMGAAp06dorW1FYfDwcGDB/nDH/5w2bO9F//j/vnPf3Lu3DnCw8ODy+bNm0daWhoHDhzgN7/5Dbm5ub0xJOlDLMti3bp1ZGRk8NxzzwFw4sQJ9u698MWuQCDA3//+d2JiYjh06BATJ07stM39+/cTFxdHZWUljz/+ODabjZCQELKysoIH/crKSmw2G1OmTOF73/sev/jFL0hLS8Nut/P222+TlZWlYJAgy7JYv34999xzDz/72c+CgbBjxw6GDx/OsGHD2LBhAwDHjh0jNDT0iuu89957zJs3r8Pt9YvLSj6fj6FDhzJgwAAAIiMjO/weRHl5Oenp6dx2223BN/g3JSQk4PV6u6W/0r8cOHCAsLCw4Kd2gNjYWO677z4ADh06xJgxY7j77rspLy/vUpvl5eXcd999xMTEUFtbGyyfMGECCQkJ/P73v+e9994LnnUMHz6cH/3oR+zYsYO//OUvjB07lltuueU6jlL6uwMHDjBw4ECmT58OQEhICE899RRlZWU0NDRcdkyMi4tjwIABV1znr3/9K19//XWH2+sX4XDbbbfh8Xh47rnnyM/P59ChQx3Wr6ioYNq0aUybNo09e/a0W8ftdpOamtod3ZV+5ssvv2TcuHFXXL5nzx6mTZvGHXfcQXV1Na2trR2219LSwj/+8Q+cTifTpk0zAuXxxx/n448/5q677mLUqFHB8rvvvpv6+np2797N3Llzr21QcsNpbz+NiIggNjaW+++/n48++ojVq1fz/vvv09DQ0OE6MTExHD9+vMPt9YtwCA8P59VXX2XhwoVERkaSk5NDSUlJu3U///xzIiMjiYmJISkpiS+++AK/3x9c/rvf/Y7nnnuO3NxcHnzwwR4agfQn+fn5rFixglWrVtHa2kpNTQ2pqalEREQwYcIE9u3b1+H61dXVJCYmMnDgQO68806qqqoIBALB5Rfve3355ZeXrRcSEoLL5SIlJYWhQ4d2y9jkxrV582YeeOAB/H4/q1atuuZftu43FzRDQkJITEwkMTGRsWPHUlJSQmZmplGvvLyc//znPzz77LMANDc3U1lZGfxiyMV7Dn/84x954403ePXVV3tyGNIHjRkzhr/97W/B11lZWZw6dYpVq1bhdrs5e/Ysy5cvB+Drr79m4MCBTJ48+Yrt7dmzh8OHDwf3wdOnT3PgwAEmTZrEuXPnePfdd/nlL3/Jli1bqK6u5vbbbw+ue/GBC5Fvuummmy7bTwHOnj3LV199RVxcXPDDyJ133onNZqOmpobvfOc77a5z8uTJy85a29MvzhyOHTsWPE0C+OKLL4iNjTXqBQIBPvnkE9avX09eXh55eXmsWLGi3evE9957L5Zl4Xa7u7Xv0vdNnDiR8+fP8+c//zlY1tLSAlz4sLFo0aLg/rR582b2799/xeu1Z8+e5bPPPmPLli3BdRYsWBC8vLlz506mTJnC6NGjycrKYvv27cFtiXQkKSmJr7/+mtLSUuDC8e63v/0t9957L0ePHg1eIWltbaW+vp7Y2NgrrpOZmcmgQYM63F6/OHM4d+4cb7/9NmfOnCE0NJRRo0axcOFCo95nn32Gw+G47MbMrbfeyuuvv47P57usrs1mY86cOezevZvk5ORuH4P0XTabjRUrVrB9+3Y++ugjIiMjCQ8P55FHHmH79u0888wzwbrh4eHccsstfPrpp8CFx1kv/dsjP/7xj5k4cWLw4QmA1NRUduzYwRdffEFVVRXr1q0DYNy4cdx222189NFHPPLIIz00WumvbDYby5cvp6CggA8++IBTp04xdepU5syZQ2lpKfn5+ViWFXxc9eIZxPLly8nPz+eDDz7AsixSUlJ47LHHOt+efj5DRKT/OXz4MK+//jrLly8nPj7+urevcBAREUO/uOcgIiI9S+EgIiIGhYOIiBgUDiIiYlA4iIiIQeEgIiKG/wfHcfRqWxOO4gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ9bQLYAexaK",
        "colab_type": "text"
      },
      "source": [
        "Clearly the dataset is not balanced , but we will continue and see how it will affect the classification of the celestial objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUVV_8W7fJe6",
        "colab_type": "text"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ1GdoJrpx1c",
        "colab_type": "text"
      },
      "source": [
        "###Seperating the matrix of features and the dependent variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnxFz_fOe7hJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=dataset.iloc[:,[7]].values\n",
        "dataset.drop(columns='class',inplace=True)\n",
        "X=dataset.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyM0ZOiKnR8v",
        "colab_type": "code",
        "outputId": "4d17e038-07a6-4b18-d93d-e5f100f01662",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['STAR'],\n",
              "       ['STAR'],\n",
              "       ['STAR'],\n",
              "       ...,\n",
              "       ['GALAXY'],\n",
              "       ['GALAXY'],\n",
              "       ['STAR']], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg-Wes-GnrCL",
        "colab_type": "code",
        "outputId": "68665812-6166-4923-b7a4-4c97120fd9f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "X"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4.73725449e+01,  8.20620553e-01,  1.86925400e+01, ...,\n",
              "         1.63466200e+01,  1.61763900e+01,  1.14653600e-04],\n",
              "       [ 1.16303083e+02,  4.24559804e+01,  1.84763300e+01, ...,\n",
              "         1.73278000e+01,  1.73711400e+01, -9.27588800e-05],\n",
              "       [ 1.72756623e+02, -8.78569788e+00,  1.64771400e+01, ...,\n",
              "         1.57220700e+01,  1.58247100e+01,  1.64580700e-04],\n",
              "       ...,\n",
              "       [ 2.05426531e+02,  3.84990534e+01,  1.75069000e+01, ...,\n",
              "         1.50446900e+01,  1.52866800e+01,  4.58666300e-03],\n",
              "       [ 3.37135144e+02, -9.63596714e+00,  1.93394600e+01, ...,\n",
              "         1.58674500e+01,  1.55155600e+01,  8.45376500e-02],\n",
              "       [ 3.74401880e+01,  2.54008880e+01,  1.81176900e+01, ...,\n",
              "         1.60758200e+01,  1.59470100e+01, -8.46093100e-05]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iqM_Rj1s6zf",
        "colab_type": "text"
      },
      "source": [
        "###Encoding the dependent variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-nCj35lntO4",
        "colab_type": "code",
        "outputId": "4dd78561-da1a-4000-e52b-9f81e71ff7d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder=LabelEncoder()\n",
        "y=label_encoder.fit_transform(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjKDqQUluVe0",
        "colab_type": "code",
        "outputId": "1219b4d7-334f-4381-fed5-be10d14c44aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 2, ..., 0, 0, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdkt1iARutj8",
        "colab_type": "text"
      },
      "source": [
        "###Splitting the dataset into training set and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMDqVyJyuV8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPnXmh5fvRqE",
        "colab_type": "text"
      },
      "source": [
        "### Applying feature scaling to ease computation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15n2poEBvPFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X=StandardScaler()\n",
        "X_train=sc_X.fit_transform(X_train)\n",
        "X_test=sc_X.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ca6v_eXv7qi",
        "colab_type": "text"
      },
      "source": [
        "#Designing the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJf7Mg7Ovz_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout\n",
        "import keras "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyS1LTx8wQSM",
        "colab_type": "text"
      },
      "source": [
        "**Iterative program  to give the neural network:**\n",
        "The objective of the following iteration is to see the effect the adding hidden layers to neural network. We have defined a list that contains the number of hidden layers to be added in each iteration. Essentially this program creates diffrent neural networks with different number of hidden layers ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcsQ5mcy3DaU",
        "colab_type": "code",
        "outputId": "ac03aeab-0ba6-460b-918d-74110c36fc61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "layers=[1,3,5,7,9,10]\n",
        "\n",
        "for i in range(len(layers)):\n",
        "  print(\"This is the \",i+1,\" iteration for finding the better neural network.\")\n",
        "  classifier=Sequential() #initialising the neural network\n",
        "  classifier.add(Dense(output_dim=5,init='uniform',activation='relu',input_dim=8)) #making the input layer\n",
        "  hidden_layers=layers[i]\n",
        "\n",
        "  for i in range(hidden_layers):\n",
        "    classifier.add(Dense(output_dim=10,init='uniform',activation='relu')) #iteration for adding different numbers of hidden layers\n",
        "    classifier.add(Dropout(0.1)) #dropout , to randomly remove neurons in each layer so as to avoid overfitting\n",
        "\n",
        "  classifier.add(Dense(output_dim=3,init='uniform',activation='softmax')) #making the output layer\n",
        "  optimizer=keras.optimizers.Adam(lr=0.001,decay=1e-6) #defining the optimizer \n",
        "\n",
        "  classifier.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer,metrics=['accuracy']) #compiling the neural network\n",
        "  classifier.fit(X_train,y_train,batch_size=10,epochs=10,validation_data=(X_test,y_test)) #fitting the classifier"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is the  1 th iteration for finding the better neural network.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=8, units=5, kernel_initializer=\"uniform\")`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=10, kernel_initializer=\"uniform\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=3, kernel_initializer=\"uniform\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 80000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "80000/80000 [==============================] - 36s 448us/step - loss: 0.5013 - accuracy: 0.7931 - val_loss: 0.1662 - val_accuracy: 0.9647\n",
            "Epoch 2/10\n",
            "80000/80000 [==============================] - 34s 420us/step - loss: 0.1616 - accuracy: 0.9558 - val_loss: 0.0996 - val_accuracy: 0.9761\n",
            "Epoch 3/10\n",
            "80000/80000 [==============================] - 35s 436us/step - loss: 0.1138 - accuracy: 0.9735 - val_loss: 0.0915 - val_accuracy: 0.9754\n",
            "Epoch 4/10\n",
            "80000/80000 [==============================] - 34s 422us/step - loss: 0.1008 - accuracy: 0.9755 - val_loss: 0.0843 - val_accuracy: 0.9791\n",
            "Epoch 5/10\n",
            "80000/80000 [==============================] - 33s 415us/step - loss: 0.0931 - accuracy: 0.9770 - val_loss: 0.0718 - val_accuracy: 0.9843\n",
            "Epoch 6/10\n",
            "80000/80000 [==============================] - 34s 422us/step - loss: 0.0886 - accuracy: 0.9783 - val_loss: 0.0686 - val_accuracy: 0.9829\n",
            "Epoch 7/10\n",
            "80000/80000 [==============================] - 35s 431us/step - loss: 0.0854 - accuracy: 0.9782 - val_loss: 0.0658 - val_accuracy: 0.9844\n",
            "Epoch 8/10\n",
            "80000/80000 [==============================] - 33s 416us/step - loss: 0.0825 - accuracy: 0.9793 - val_loss: 0.0616 - val_accuracy: 0.9858\n",
            "Epoch 9/10\n",
            "80000/80000 [==============================] - 34s 419us/step - loss: 0.0795 - accuracy: 0.9800 - val_loss: 0.0592 - val_accuracy: 0.9858\n",
            "Epoch 10/10\n",
            "80000/80000 [==============================] - 33s 418us/step - loss: 0.0789 - accuracy: 0.9806 - val_loss: 0.0749 - val_accuracy: 0.9804\n",
            "This is the  2 th iteration for finding the better neural network.\n",
            "Train on 80000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "80000/80000 [==============================] - 42s 526us/step - loss: 0.6641 - accuracy: 0.7223 - val_loss: 0.5619 - val_accuracy: 0.7910\n",
            "Epoch 2/10\n",
            "80000/80000 [==============================] - 42s 530us/step - loss: 0.5446 - accuracy: 0.8092 - val_loss: 0.3359 - val_accuracy: 0.9075\n",
            "Epoch 3/10\n",
            "80000/80000 [==============================] - 42s 523us/step - loss: 0.3701 - accuracy: 0.8946 - val_loss: 0.2187 - val_accuracy: 0.9424\n",
            "Epoch 4/10\n",
            "80000/80000 [==============================] - 42s 522us/step - loss: 0.2599 - accuracy: 0.9273 - val_loss: 0.1376 - val_accuracy: 0.9617\n",
            "Epoch 5/10\n",
            "80000/80000 [==============================] - 42s 530us/step - loss: 0.1799 - accuracy: 0.9527 - val_loss: 0.0853 - val_accuracy: 0.9800\n",
            "Epoch 6/10\n",
            "80000/80000 [==============================] - 42s 526us/step - loss: 0.1294 - accuracy: 0.9658 - val_loss: 0.0685 - val_accuracy: 0.9840\n",
            "Epoch 7/10\n",
            "80000/80000 [==============================] - 42s 521us/step - loss: 0.1166 - accuracy: 0.9691 - val_loss: 0.0624 - val_accuracy: 0.9840\n",
            "Epoch 8/10\n",
            "80000/80000 [==============================] - 42s 526us/step - loss: 0.1075 - accuracy: 0.9709 - val_loss: 0.0576 - val_accuracy: 0.9857\n",
            "Epoch 9/10\n",
            "80000/80000 [==============================] - 43s 533us/step - loss: 0.1051 - accuracy: 0.9713 - val_loss: 0.0616 - val_accuracy: 0.9858\n",
            "Epoch 10/10\n",
            "80000/80000 [==============================] - 42s 523us/step - loss: 0.1018 - accuracy: 0.9727 - val_loss: 0.0697 - val_accuracy: 0.9808\n",
            "This is the  3 th iteration for finding the better neural network.\n",
            "Train on 80000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "80000/80000 [==============================] - 50s 619us/step - loss: 0.4800 - accuracy: 0.8214 - val_loss: 0.1828 - val_accuracy: 0.9772\n",
            "Epoch 2/10\n",
            "80000/80000 [==============================] - 50s 622us/step - loss: 0.2387 - accuracy: 0.9505 - val_loss: 0.1450 - val_accuracy: 0.9758\n",
            "Epoch 3/10\n",
            "80000/80000 [==============================] - 50s 619us/step - loss: 0.1981 - accuracy: 0.9549 - val_loss: 0.1221 - val_accuracy: 0.9785\n",
            "Epoch 4/10\n",
            "80000/80000 [==============================] - 49s 617us/step - loss: 0.1691 - accuracy: 0.9642 - val_loss: 0.1010 - val_accuracy: 0.9801\n",
            "Epoch 5/10\n",
            "80000/80000 [==============================] - 49s 615us/step - loss: 0.1395 - accuracy: 0.9698 - val_loss: 0.0950 - val_accuracy: 0.9796\n",
            "Epoch 6/10\n",
            "80000/80000 [==============================] - 50s 630us/step - loss: 0.1306 - accuracy: 0.9718 - val_loss: 0.1155 - val_accuracy: 0.9619\n",
            "Epoch 7/10\n",
            "80000/80000 [==============================] - 49s 616us/step - loss: 0.1209 - accuracy: 0.9745 - val_loss: 0.0882 - val_accuracy: 0.9802\n",
            "Epoch 8/10\n",
            "80000/80000 [==============================] - 50s 622us/step - loss: 0.1140 - accuracy: 0.9751 - val_loss: 0.0631 - val_accuracy: 0.9841\n",
            "Epoch 9/10\n",
            "80000/80000 [==============================] - 49s 616us/step - loss: 0.1067 - accuracy: 0.9765 - val_loss: 0.0629 - val_accuracy: 0.9853\n",
            "Epoch 10/10\n",
            "80000/80000 [==============================] - 49s 616us/step - loss: 0.1030 - accuracy: 0.9770 - val_loss: 0.0851 - val_accuracy: 0.9794\n",
            "This is the  4 th iteration for finding the better neural network.\n",
            "Train on 80000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "80000/80000 [==============================] - 58s 728us/step - loss: 0.6688 - accuracy: 0.7211 - val_loss: 0.5019 - val_accuracy: 0.8291\n",
            "Epoch 2/10\n",
            "80000/80000 [==============================] - 59s 732us/step - loss: 0.3580 - accuracy: 0.9020 - val_loss: 0.1643 - val_accuracy: 0.9724\n",
            "Epoch 3/10\n",
            "80000/80000 [==============================] - 58s 727us/step - loss: 0.2349 - accuracy: 0.9420 - val_loss: 0.1379 - val_accuracy: 0.9764\n",
            "Epoch 4/10\n",
            "80000/80000 [==============================] - 58s 730us/step - loss: 0.2206 - accuracy: 0.9446 - val_loss: 0.2061 - val_accuracy: 0.9600\n",
            "Epoch 5/10\n",
            "80000/80000 [==============================] - 58s 728us/step - loss: 0.2247 - accuracy: 0.9414 - val_loss: 0.1399 - val_accuracy: 0.9737\n",
            "Epoch 6/10\n",
            "80000/80000 [==============================] - 58s 725us/step - loss: 0.2092 - accuracy: 0.9467 - val_loss: 0.1575 - val_accuracy: 0.9704\n",
            "Epoch 7/10\n",
            "80000/80000 [==============================] - 59s 736us/step - loss: 0.2150 - accuracy: 0.9453 - val_loss: 0.1284 - val_accuracy: 0.9741\n",
            "Epoch 8/10\n",
            "80000/80000 [==============================] - 58s 729us/step - loss: 0.1944 - accuracy: 0.9507 - val_loss: 0.1164 - val_accuracy: 0.9796\n",
            "Epoch 9/10\n",
            "80000/80000 [==============================] - 58s 729us/step - loss: 0.1840 - accuracy: 0.9549 - val_loss: 0.1123 - val_accuracy: 0.9785\n",
            "Epoch 10/10\n",
            "80000/80000 [==============================] - 58s 729us/step - loss: 0.1659 - accuracy: 0.9617 - val_loss: 0.0922 - val_accuracy: 0.9811\n",
            "This is the  5 th iteration for finding the better neural network.\n",
            "Train on 80000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "80000/80000 [==============================] - 67s 838us/step - loss: 0.6642 - accuracy: 0.7032 - val_loss: 0.5329 - val_accuracy: 0.7883\n",
            "Epoch 2/10\n",
            "80000/80000 [==============================] - 67s 835us/step - loss: 0.5537 - accuracy: 0.7673 - val_loss: 0.4856 - val_accuracy: 0.8007\n",
            "Epoch 3/10\n",
            "80000/80000 [==============================] - 66s 830us/step - loss: 0.5168 - accuracy: 0.7947 - val_loss: 0.4671 - val_accuracy: 0.8169\n",
            "Epoch 4/10\n",
            "80000/80000 [==============================] - 66s 830us/step - loss: 0.4651 - accuracy: 0.8385 - val_loss: 0.2511 - val_accuracy: 0.9410\n",
            "Epoch 5/10\n",
            "80000/80000 [==============================] - 66s 828us/step - loss: 0.2547 - accuracy: 0.9273 - val_loss: 0.1491 - val_accuracy: 0.9643\n",
            "Epoch 6/10\n",
            "80000/80000 [==============================] - 66s 826us/step - loss: 0.1940 - accuracy: 0.9436 - val_loss: 0.1369 - val_accuracy: 0.9611\n",
            "Epoch 7/10\n",
            "80000/80000 [==============================] - 67s 838us/step - loss: 0.1704 - accuracy: 0.9534 - val_loss: 0.1159 - val_accuracy: 0.9709\n",
            "Epoch 8/10\n",
            "80000/80000 [==============================] - 66s 830us/step - loss: 0.1572 - accuracy: 0.9621 - val_loss: 0.0997 - val_accuracy: 0.9758\n",
            "Epoch 9/10\n",
            "80000/80000 [==============================] - 66s 826us/step - loss: 0.1519 - accuracy: 0.9629 - val_loss: 0.1040 - val_accuracy: 0.9761\n",
            "Epoch 10/10\n",
            "80000/80000 [==============================] - 66s 826us/step - loss: 0.1465 - accuracy: 0.9637 - val_loss: 0.0923 - val_accuracy: 0.9786\n",
            "This is the  6 th iteration for finding the better neural network.\n",
            "Train on 80000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "80000/80000 [==============================] - 71s 885us/step - loss: 0.6461 - accuracy: 0.7287 - val_loss: 0.2919 - val_accuracy: 0.9599\n",
            "Epoch 2/10\n",
            "80000/80000 [==============================] - 70s 875us/step - loss: 0.2800 - accuracy: 0.9365 - val_loss: 0.2180 - val_accuracy: 0.9525\n",
            "Epoch 3/10\n",
            "80000/80000 [==============================] - 71s 884us/step - loss: 0.2253 - accuracy: 0.9490 - val_loss: 0.1672 - val_accuracy: 0.9669\n",
            "Epoch 4/10\n",
            "80000/80000 [==============================] - 70s 878us/step - loss: 0.2073 - accuracy: 0.9545 - val_loss: 0.1314 - val_accuracy: 0.9762\n",
            "Epoch 5/10\n",
            "80000/80000 [==============================] - 70s 879us/step - loss: 0.2059 - accuracy: 0.9551 - val_loss: 0.1283 - val_accuracy: 0.9733\n",
            "Epoch 6/10\n",
            "80000/80000 [==============================] - 70s 880us/step - loss: 0.2011 - accuracy: 0.9554 - val_loss: 0.1387 - val_accuracy: 0.9768\n",
            "Epoch 7/10\n",
            "80000/80000 [==============================] - 70s 876us/step - loss: 0.1878 - accuracy: 0.9592 - val_loss: 0.1235 - val_accuracy: 0.9778\n",
            "Epoch 8/10\n",
            "80000/80000 [==============================] - 70s 875us/step - loss: 0.1913 - accuracy: 0.9578 - val_loss: 0.1162 - val_accuracy: 0.9799\n",
            "Epoch 9/10\n",
            "80000/80000 [==============================] - 70s 874us/step - loss: 0.1776 - accuracy: 0.9588 - val_loss: 0.1355 - val_accuracy: 0.9701\n",
            "Epoch 10/10\n",
            "80000/80000 [==============================] - 71s 883us/step - loss: 0.1716 - accuracy: 0.9613 - val_loss: 0.0999 - val_accuracy: 0.9798\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li3ujyGoLITd",
        "colab_type": "text"
      },
      "source": [
        "Just to be sure that our model is working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KdKAtb_P_zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred=classifier.predict(X_test)\n",
        "predictions=[]\n",
        "for i in y_pred:\n",
        "  l=i.tolist()\n",
        "  confidence=max(l)\n",
        "  classification=l.index(confidence)\n",
        "  predictions.append(classification)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhYx5wFPSxNV",
        "colab_type": "text"
      },
      "source": [
        "Making the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "993BelXKSWgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test,predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey8C0S0MSndd",
        "colab_type": "code",
        "outputId": "232ae654-33b7-4609-d492-a6d45f716fc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "cm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[10023,    27,   203],\n",
              "       [   89,  2015,    20],\n",
              "       [   18,    46,  7559]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVXaSTlKS2Hz",
        "colab_type": "text"
      },
      "source": [
        "We can see from the confusion matrix that the accuracy is about **97.985%** , which is the validation accuracy for the neural network with the 10 hidden layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8hjkJljT5dy",
        "colab_type": "text"
      },
      "source": [
        "We can also see that the best accuracy is given by the neural network with **7 hidden layers**. But there is a very small distinction between the accuracy of of a neural network with one hidden layer and seven hidden layers . "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfxY8rZsUgMG",
        "colab_type": "text"
      },
      "source": [
        "#Visualising the neural network\n",
        "We can visualise the neural network to get a sense of it's structure and  improve the interpretation of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruyPfZK8U8P8",
        "colab_type": "code",
        "outputId": "cd8f79bd-2471-4779-b16e-1492d07821c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 840
        }
      },
      "source": [
        "from keras_sequential_ascii import keras2ascii\n",
        "keras2ascii(classifier)\n",
        "print('\\n The above diagram is the final iteration of the neural network iteration loop. It has 10 hidden layers.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
            "\n",
            "               Input   #####           8\n",
            "               Dense   XXXXX -------------------        45     4.0%\n",
            "                relu   #####           5\n",
            "               Dense   XXXXX -------------------        60     5.3%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110     9.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110     9.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110     9.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110     9.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110     9.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110     9.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110     9.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110     9.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110     9.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------        33     2.9%\n",
            "             softmax   #####           3\n",
            "\n",
            " The above diagram is the final iteration of the neural network iteration loop. It has 10 hidden layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh57NP0-mJWk",
        "colab_type": "text"
      },
      "source": [
        "#Visualising all the neural networks created during the iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PFN6a1ImSMw",
        "colab_type": "code",
        "outputId": "a74e2abf-0fee-4747-837f-81591959e598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "layers=[1,3,5,7,9,10]\n",
        "\n",
        "for i in range(len(layers)):\n",
        "  print(\" \\n This is iteration-\",i+1,\" \\n \")\n",
        "  classifier=Sequential() #initialising the neural network\n",
        "  classifier.add(Dense(output_dim=5,init='uniform',activation='relu',input_dim=8)) #making the input layer\n",
        "  hidden_layers=layers[i]\n",
        "\n",
        "  for i in range(hidden_layers):\n",
        "    classifier.add(Dense(output_dim=10,init='uniform',activation='relu')) #iteration for adding different numbers of hidden layers\n",
        "    classifier.add(Dropout(0.1)) #dropout , to randomly remove neurons in each layer so as to avoid overfitting\n",
        "\n",
        "  classifier.add(Dense(output_dim=3,init='uniform',activation='softmax')) #making the output layer\n",
        "  optimizer=keras.optimizers.Adam(lr=0.001,decay=1e-6) #defining the optimizer \n",
        "\n",
        "  classifier.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer,metrics=['accuracy']) #compiling the neural network\n",
        "  keras2ascii(classifier)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            " This is iteration- 1  \n",
            " \n",
            "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
            "\n",
            "               Input   #####           8\n",
            "               Dense   XXXXX -------------------        45    32.6%\n",
            "                relu   #####           5\n",
            "               Dense   XXXXX -------------------        60    43.5%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------        33    23.9%\n",
            "             softmax   #####           3\n",
            " \n",
            " This is iteration- 2  \n",
            " \n",
            "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
            "\n",
            "               Input   #####           8\n",
            "               Dense   XXXXX -------------------        45    12.6%\n",
            "                relu   #####           5\n",
            "               Dense   XXXXX -------------------        60    16.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110    30.7%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110    30.7%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------        33     9.2%\n",
            "             softmax   #####           3\n",
            " \n",
            " This is iteration- 3  \n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=8, units=5, kernel_initializer=\"uniform\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=10, kernel_initializer=\"uniform\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=3, kernel_initializer=\"uniform\")`\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
            "\n",
            "               Input   #####           8\n",
            "               Dense   XXXXX -------------------        45     7.8%\n",
            "                relu   #####           5\n",
            "               Dense   XXXXX -------------------        60    10.4%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110    19.0%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110    19.0%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110    19.0%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110    19.0%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------        33     5.7%\n",
            "             softmax   #####           3\n",
            " \n",
            " This is iteration- 4  \n",
            " \n",
            "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
            "\n",
            "               Input   #####           8\n",
            "               Dense   XXXXX -------------------        45     5.6%\n",
            "                relu   #####           5\n",
            "               Dense   XXXXX -------------------        60     7.5%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110    13.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110    13.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110    13.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110    13.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110    13.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110    13.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------        33     4.1%\n",
            "             softmax   #####           3\n",
            " \n",
            " This is iteration- 5  \n",
            " \n",
            "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
            "\n",
            "               Input   #####           8\n",
            "               Dense   XXXXX -------------------        45     4.4%\n",
            "                relu   #####           5\n",
            "               Dense   XXXXX -------------------        60     5.9%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110    10.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110    10.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110    10.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110    10.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110    10.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110    10.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110    10.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110    10.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------        33     3.2%\n",
            "             softmax   #####           3\n",
            " \n",
            " This is iteration- 6  \n",
            " \n",
            "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
            "\n",
            "               Input   #####           8\n",
            "               Dense   XXXXX -------------------        45     4.0%\n",
            "                relu   #####           5\n",
            "               Dense   XXXXX -------------------        60     5.3%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110     9.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110     9.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110     9.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110     9.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110     9.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110     9.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110     9.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110     9.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------       110     9.8%\n",
            "                relu   #####          10\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####          10\n",
            "               Dense   XXXXX -------------------        33     2.9%\n",
            "             softmax   #####           3\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}